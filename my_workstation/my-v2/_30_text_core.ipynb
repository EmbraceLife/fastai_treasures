{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text core\n",
    "\n",
    "> Basic function to preprocess text before assembling it in a `DataBunch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from multiprocessing import Process, Queue\n",
    "import spacy,html\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are rules applied to texts before or after it's tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxfld xxrep xxwrep xxup xxmaj\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_spec = re.compile(r'([/#\\\\])')\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return _re_spec.sub(r' \\1 ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(spec_add_spaces('#fastai'), ' # fastai')\n",
    "test_eq(spec_add_spaces('/fastai'), ' / fastai')\n",
    "test_eq(spec_add_spaces('\\\\fastai'), ' \\\\ fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_space = re.compile(' {2,}')\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(rm_useless_spaces('a  b   c'), 'a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    return _re_rep.sub(_replace_rep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same character or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_rep('aaa'), 'aaa')\n",
    "test_eq(replace_rep('aaaa'), f' {TK_REP} 4 a ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_wrep = re.compile(r'(?:\\s|^)(\\w+)\\s+((?:\\1\\s+){2,})\\1(\\s|\\W|$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Matches any word repeated at least four times with spaces between them\n",
    "```\n",
    "(?:\\s|^)          Non-catching group with either a whitespace character or the beginning of text\n",
    "(\\w+)             Catching group of any alphanumeric character\n",
    "\\s+               One or more whitespace\n",
    "((?:\\1\\s+){2,})   Catching group of a repetition of two or more times \\1 followed by one or more whitespace\n",
    "\\1                Occurence of \\1\n",
    "(\\s|\\W|$)         Catching group of last whitespace, non alphanumeric character or end of text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word word -> TK_WREP 4 word\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc,e = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+2} {c} {e}'\n",
    "    return _re_wrep.sub(_replace_wrep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same word or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_wrep('ah ah'), 'ah ah')\n",
    "test_eq(replace_wrep('ah ah ah ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah   ah  ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah ah ah '), f' {TK_WREP} 4 ah  ')\n",
    "test_eq(replace_wrep('ah ah ah ah.'), f' {TK_WREP} 4 ah .')\n",
    "test_eq(replace_wrep('ah ah ah ahi'), f'ah ah ah ahi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_html(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n",
    "        '#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(' @-@ ','-')\n",
    "    return html.unescape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fix_html('#39;bli#146;'), \"'bli'\")\n",
    "test_eq(fix_html('Sarah amp; Duck'), 'Sarah & Duck')\n",
    "test_eq(fix_html('a nbsp; #36;'), 'a   $')\n",
    "test_eq(fix_html('\\\\\" <unk>'), f'\" {UNK}')\n",
    "test_eq(fix_html('quot;  @.@  @-@ '), \"' .-\")\n",
    "test_eq(fix_html('<br />text\\\\n'), '\\ntext\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_all_caps = re.compile(r'(\\s|^)([A-Z]+[^a-z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any word in all caps, even with ' or - inside\n",
    "```\n",
    "(\\s|^)        Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]+       Catching group with one capitalized letter or more...\n",
    "[^a-z\\s]*)    ...followed by anything that's non lowercase or whitespace\n",
    "(?=(\\s|$))    Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_all_caps(m):\n",
    "        tok = f'{TK_UP} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_all_caps.sub(_replace_all_caps, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_all_caps(\"I'M SHOUTING\"), f\"{TK_UP} i'm {TK_UP} shouting\")\n",
    "test_eq(replace_all_caps(\"I'm speaking normally\"), \"I'm speaking normally\")\n",
    "test_eq(replace_all_caps(\"I am speaking normally\"), \"i am speaking normally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_maj = re.compile(r'(\\s|^)([A-Z][^A-Z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any capitalized word\n",
    "```\n",
    "(\\s|^)       Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]       Catching group with exactly one capitalized letter...\n",
    "[^A-Z\\s]*)   ...followed by anything that's not uppercase or whitespace\n",
    "(?=(\\s|$))   Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_maj(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_maj(m):\n",
    "        tok = f'{TK_MAJ} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_maj.sub(_replace_maj, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_maj(\"Jeremy Howard\"), f'{TK_MAJ} jeremy {TK_MAJ} howard')\n",
    "test_eq(replace_maj(\"I don't think there is any maj here\"), (\"i don't think there is any maj here\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lowercase(t, add_bos=True, add_eos=False):\n",
    "    \"Converts `t` to lowercase\"\n",
    "    return (f'{BOS} ' if add_bos else '') + t.lower().strip() + (f' {EOS}' if add_eos else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.text_spec_tok = [UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "defaults.text_proc_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces,\n",
    "                            replace_all_caps, replace_maj, lowercase]\n",
    "defaults.text_token_sep = '\\u2581'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is a class that must implement a `pipe` method. This `pipe` method receives a generator of texts and must return a generator with their tokenized versions. Here is the most basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseTokenizer():\n",
    "    \"Basic tokenizer that just splits on spaces\"\n",
    "    def __init__(self, **kwargs): pass\n",
    "    def pipe(self, items): \n",
    "        for t in items: yield t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BaseTokenizer()\n",
    "for t in tok.pipe([\"This is a text\"]): test_eq(t, [\"This\", \"is\", \"a\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpacyTokenizer():\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, batch_size=5000):\n",
    "        special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        self.nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in special_toks: self.nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def pipe(self, items):\n",
    "        for doc in self.nlp.pipe(items, batch_size=self.batch_size):\n",
    "            yield [d.text for d in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = SpacyTokenizer()\n",
    "for t in tok.pipe([\"This isn't the easiest text.\"]): \n",
    "    test_eq(t, [\"This\", \"is\", \"n't\", \"the\", \"easiest\", \"text\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_rules(items, rules):\n",
    "    \"Returns a generator that apply `rules`  to `items`\"\n",
    "    for o in items: yield compose(*rules)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in apply_rules([\"This is a text\"], [replace_maj]): test_eq(t, f\"{TK_MAJ} this is a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize1(text, tok_func=SpacyTokenizer, rules=None, **tok_kwargs):\n",
    "    \"Tokenize one `text` with an instance of `tok_func` and some `rules`\"\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules))\n",
    "    tokenizer = tok_func(**tok_kwargs)\n",
    "    for tok in tokenizer.pipe(apply_rules([text], rules)): return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tokenize1(\"This is a text\"), [BOS, TK_MAJ, 'this', 'is', 'a', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_items(items, tok_func, rules, output_func, output_queue, data_queue=None, **tok_kwargs):\n",
    "    'Tokenize `items` with an instance of `tok_func` and some `rules`'\n",
    "    tokenizer = tok_func(**tok_kwargs)\n",
    "    if data_queue: counts = Counter()\n",
    "    for o,tok in zip(items, tokenizer.pipe(apply_rules(items, rules))):\n",
    "        output_queue.put(output_func(o, tok))\n",
    "        if data_queue: counts.update(Counter(tok))\n",
    "    if data_queue: data_queue.put(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that will be called during one of the processes handling tokenization. It will create an instance of a tokenizer with `tok_func` and `tok_kwargs`, then iterate through the `items`, apply them `rules`, tokenize them, then apply `output_func` to the original item and the tokens and put the result in `output_queue`.\n",
    "\n",
    "If a `data_queue` is passed, we count the different tokens and return the Counter in it at the end, to save the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_process(items, tok_func, rules, output_func):#Just for testing\n",
    "    output_queue = Queue(1)\n",
    "    process = Process(target=tokenize_items, args=(items, tok_func, rules, output_func, output_queue))\n",
    "    process.start()\n",
    "    for _ in range(2): print(output_queue.get())\n",
    "    process.join()\n",
    "\n",
    "texts = [\"this is a text\", \"this is another text\"]\n",
    "rules = [lambda i: texts[i]]\n",
    "output_func = lambda i,t: t\n",
    "test_stdout(lambda: launch_process([0,1], BaseTokenizer, rules, output_func),\n",
    "           \"\"\"['this', 'is', 'a', 'text']\n",
    "['this', 'is', 'another', 'text']\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_folders(path, output_dir, include=None):\n",
    "    \"Scan `path` and create the same folder architecture in `output_dir`\"\n",
    "    output_dir = Path(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "        if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "        else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "        for x in d: os.makedirs(output_dir/(Path(p)/Path(x)).relative_to(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `include` is specified, only the folders in `include` will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for k in range(i+1): os.makedirs(path/d/str(k), exist_ok=True)\n",
    "create_folders(path, 'tmp1')\n",
    "out = Path('tmp1')\n",
    "assert out.is_dir()\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    assert (out/d).is_dir()\n",
    "    for k in range(i+1): assert (out/d/str(k)).is_dir()\n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test with include\n",
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for k in range(i+1): os.makedirs(path/d/str(k), exist_ok=True)\n",
    "create_folders(path, 'tmp1', include=['b', 'c'])\n",
    "out = Path('tmp1')\n",
    "assert out.is_dir()\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    if i==0:\n",
    "        assert not (out/d).is_dir()\n",
    "        continue\n",
    "    assert (out/d).is_dir()\n",
    "    for k in range(i+1): assert (out/d/str(k)).is_dir()\n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function for texts in filenames. Tokenized texts will be saved in a similar fashion in a directory suffixed with `_tok` in the parent folder of `path` (override with `output_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_text(fname):\n",
    "    \"Read the content of `fname`\"\n",
    "    with open(fname, 'r') as f: return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_folder(path, extensions=None, include=None, output_dir=None, n_workers=defaults.cpus,\n",
    "                    rules=None, tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize text files in `path` in parallel using `n_workers`\"\n",
    "    path = Path(path)\n",
    "    extensions = ifnone(extensions, ['.txt'])\n",
    "    fnames = get_files(path, extensions=extensions, recurse=True, include=include)\n",
    "    output_dir = Path(ifnone(output_dir, path.parent/f'{path.name}_tok'))\n",
    "    create_folders(path, output_dir, include=include)\n",
    "    rules = read_text + L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    \n",
    "    output_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    def _output(o, tok):\n",
    "        out = output_dir/o.relative_to(path)\n",
    "        with open(out, 'w') as f: f.write(defaults.text_token_sep.join(tok))\n",
    "        with open(out.parent/f'{out.stem}.len', 'w') as f: f.write(str(len(tok)))\n",
    "        return 1\n",
    "            \n",
    "    processes = [Process(target=tokenize_items,\n",
    "                         args=(batch, tok_func, rules, _output, output_queue),\n",
    "                         kwargs={'data_queue': data_queue, **tok_kwargs})\n",
    "                 for i,batch in enumerate(np.array_split(fnames, n_workers))]\n",
    "    \n",
    "    for p in processes: p.start()\n",
    "    counter = Counter()\n",
    "    for _ in progress_bar(fnames, leave=False): _ = output_queue.get()\n",
    "    for _ in processes: counter.update(data_queue.get())\n",
    "    for p in processes: p.join()\n",
    "    pickle.dump(counter, open(output_dir/'counter.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be in `output_dir` (defaults to a folder in the same parent directory as `path`, with `_tok` added to `path.name`) with the same structure as in `path`. Tokenized texts for a given file will be in the file having the same name in `output_dir`. Additionally, a file with a .len suffix contains the number of tokens and the count of all words is stored in `output_dir/counter.pkl`.\n",
    "\n",
    "`extensions` will default to `['.txt']` and all text files in `path` are treated unless you specify a list of folders in `include`. `tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for d in ['a', 'b', 'c']: \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for i in range(5):\n",
    "        with open(path/d/f'text{i}.txt', 'w') as f: f.write(f\"This is an example of text {d} {i}\")\n",
    "tokenize_folder(path)\n",
    "out = Path('tmp_tok')\n",
    "assert out.is_dir()\n",
    "for d in ['a', 'b', 'c']: \n",
    "    assert (out/d).is_dir()\n",
    "    for i in range(5):\n",
    "        assert (out/d/f'text{i}.txt').is_file()\n",
    "        assert (out/d/f'text{i}.len').is_file()\n",
    "        test_eq(read_text(out/d/f'text{i}.txt'), defaults.text_token_sep.join([\n",
    "            BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', d, str(i)\n",
    "        ]))\n",
    "        test_eq(read_text(out/d/f'text{i}.len'), '10')\n",
    "        \n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tookenize texts in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_texts(idx, df, mark_fields=False):\n",
    "    \"Join texts in row `idx` of `df`, marking each field with `FLD` if `mark_fields=True`\"\n",
    "    return ' '.join([(f'{FLD} {i} ' if mark_fields else '') + t for i,t in enumerate(df.iloc[int(idx)].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_df(df, text_cols, n_workers=defaults.cpus, rules=None, mark_fields=None, \n",
    "                tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize texts in `df[text_cols]` in parallel using `n_workers`\"\n",
    "    text_cols = L(text_cols)\n",
    "    mark_fields = ifnone(mark_fields, len(text_cols) > 1)\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    rules = partial(join_texts, df=df[text_cols], mark_fields=mark_fields) + rules\n",
    "    \n",
    "    output_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    def _output(o, tok): return (o,tok)\n",
    "            \n",
    "    processes = [Process(target=tokenize_items,\n",
    "                         args=(batch, tok_func, rules, _output, output_queue),\n",
    "                         kwargs={'data_queue': data_queue, **tok_kwargs})\n",
    "                 for i,batch in enumerate(np.array_split(range(len(df)), n_workers))]\n",
    "    \n",
    "    for p in processes: p.start()\n",
    "    lengths,outputs,counter = np.zeros(len(df)),np.zeros(len(df), dtype=np.object),Counter()\n",
    "    for _ in progress_bar(range(len(df)), leave=False): \n",
    "        i,tok = output_queue.get()\n",
    "        lengths[i],outputs[i] = len(tok), defaults.text_token_sep.join(tok)\n",
    "    for _ in processes: counter.update(data_queue.get())\n",
    "    for p in processes: p.join()\n",
    "    \n",
    "    other_cols = [c for c in df.columns if c not in text_cols]\n",
    "    res = df[other_cols].copy()\n",
    "    res['text'],res['text_lengths'] = outputs,lengths\n",
    "    return res, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a new dataframe with the same non-text columns, a colum named text that contains the tokenized texts and a column named text_lengths that contains their respective length. It also returns a counter of all words see to quickly build a vocabulary afterward.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"This is an example of text {i}\" for i in range(10)]\n",
    "df = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\n",
    "out,cnt = tokenize_df(df, text_cols='text')\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], defaults.text_token_sep.join([\n",
    "        BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i)\n",
    "    ]))\n",
    "    test_eq(out['text_lengths'][i], 9)\n",
    "    \n",
    "#With two columns of text, mark_fields defaults to True\n",
    "df['text1'] = df['text'].values\n",
    "out,cnt = tokenize_df(df, text_cols=['text', 'text1'])\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], defaults.text_token_sep.join([\n",
    "        BOS, FLD, '0', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i),\n",
    "             FLD, '1', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i)\n",
    "    ]))\n",
    "    test_eq(out['text_lengths'][i], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_csv(fname, text_cols, outname=None, n_workers=4, rules=None, mark_fields=None, \n",
    "                 tok_func=SpacyTokenizer, header='infer', chunksize=None, **tok_kwargs):\n",
    "    \"Tokenize texts in the `text_cols` of the csv `fname` in parallel using `n_workers`\"\n",
    "    df = pd.read_csv(fname, header=header, chunksize=chunksize)\n",
    "    outname = Path(ifnone(outname, fname.parent/f'{fname.stem}_tok.csv'))\n",
    "    kwargs = dict(n_workers=n_workers, pre_rules=pre_rules, post_rules=post_rules, \n",
    "                  mark_fields=mark_fields, tok_func=tok_func, **tok_kwargs)\n",
    "    if chunksize is None:\n",
    "        out,cnt = tok_df(df, text_cols, **kwargs)\n",
    "        out.to_csv(outname, header=header, index=False)\n",
    "    else:\n",
    "        cnt = Counter()\n",
    "        for i,dfp in enumerate(df):\n",
    "            out,c = tok_df(dfp, text_cols, **kwargs)\n",
    "            out.to_csv(outname, header=header if i==0 else None, index=False, mode='w' if i==0 else 'a')\n",
    "            cnt.update(c)\n",
    "    pickle.dump(cnt, open(outname.parent/'counter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be written in a new csv file in `outname` (defaults to the same as `fname` with the suffix `_tok.csv`) and will have the same header as the original file, the same non-text columns, a text and a text_lengths column as described in `tokenize_df`.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field.\n",
    "\n",
    "The csv file is opened with `header` and optionally with blocks of `chunksize` at a time. If this argument is passed, each chunk is processed independtly and saved in the output file to save memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
