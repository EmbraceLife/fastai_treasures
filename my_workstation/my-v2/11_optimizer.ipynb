{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "> Define the general fastai optimizer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Optimizer():\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `steppers`\"\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        steppers,params = L(steppers),L(params)\n",
    "        for step in steppers: defaults = {**getattr(step, 'defaults', {}), **defaults}\n",
    "        self.param_groups = params if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.step_func = compose(*steppers)\n",
    "        self.hypers = L({**defaults} for p in self.param_groups)\n",
    "\n",
    "    def grad_params(self):\n",
    "        \"Helper function to loop over param groups then params that have a grad\"\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero all the grad attributes of the parameters\"\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"Execute steppers on all parameters that have a grad\"\n",
    "        for p,hyper in self.grad_params(): self.step_func(p, **hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`params` will be used to create the `param_groups` of the optimizer. If it's a colection (or a generator) of parameters, it will be a `L` containing one `L` with all the parameters. To define multiple parameter groups `params` should be passed as a collection (or a generator) of `L`s.\n",
    "\n",
    "> Note: In PyTorch, `model.parameters()` returns a generator with all the parameters, that you can directly pass to `Optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer([1,2,3], noop)\n",
    "test_eq(opt.param_groups, [[1,2,3]])\n",
    "opt = Optimizer(range(3), noop)\n",
    "test_eq(opt.param_groups, [[0,1,2]])\n",
    "opt = Optimizer([[1,2],[3]], noop)\n",
    "test_eq(opt.param_groups, [[1,2],[3]])\n",
    "opt = Optimizer(([o,o+1] for o in range(0,4,2)), noop)\n",
    "test_eq(opt.param_groups, [[0,1],[2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`steppers` is a list of functions that will be composed when applying the step. For instance, you can compose a function making the SGD step, with one applying weight decay. Additionaly, each `stepper` can have a `defaults` attribute that contains hyper-parameters and their default value. Those are all gathered at initialization, and new value can be passed to overrided those defaults with the `defaults` kwargs.\n",
    "\n",
    "Once the defaults have all been pulled off, they are copied as many times as there are `param_groups` and stored in `hypers`. To apply different hyper-parameters to different groups (differential learning rates, or no weight decay for certain layers for instance), you will need to adjsut those values after the init. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_arg(p, lr=0, **kwargs): return p\n",
    "tst_arg.defaults = dict(lr=1e-2)\n",
    "\n",
    "opt = Optimizer([1,2,3], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}])\n",
    "opt = Optimizer([1,2,3], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}, {'lr': 1e-2}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to give examples of optimizer steps, we will need some steppers, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(-lr, p.grad.data)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_param(val, grad):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = sgd_step(p, 1.)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_decay(p, lr, wd, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    p.data.mul_(1 - lr*wd)\n",
    "    return p\n",
    "weight_decay.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = weight_decay(p, 1., 0.1)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def l2_reg(p, lr, wd, **kwargs):\n",
    "    \"L2 regularization as adding `wd*p` to `p.grad`\"\n",
    "    p.grad.data.add_(wd, p.data)\n",
    "    return p\n",
    "l2_reg.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = l2_reg(p, 1., 0.1)\n",
    "test_eq(p, tensor([1.]))\n",
    "test_eq(p.grad, tensor([0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Weight decay and L2 regularization is the same thing for basic SGD, but for more complex optimizers, they are very different. See [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Optimizer.step</code>\" class=\"doc_header\"><code>Optimizer.step</code><a href=\"https://nbviewer.jupyter.org/github/fastai/fastai_docs/blob/master/dev/11_optimizer.ipynb#Optimizer--\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.step</code>()\n",
       "\n",
       "Execute steppers on all parameters that have a grad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will loop over all param groups, then all parameters for which `grad` is not None and call each function in `stepper`, passing it the parameter `p` with the hyper-parameters in the corresponding dict in `hypers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test basic step\n",
    "def tst_params(): return [tst_param(i, 0.1*i) for i in range(4)]\n",
    "\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test two steps\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test None gradients are ignored\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "params[-1].grad = None\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.99, 1.98, 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test discriminative lrs\n",
    "params = tst_params()\n",
    "opt = Optimizer([params[:2], params[2:]], sgd_step, lr=0.1)\n",
    "opt.hypers[0]['lr'] = 0.01\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.999, 1.98, 2.97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Optimizer.zero_grad</code>\" class=\"doc_header\"><code>Optimizer.zero_grad</code><a href=\"https://nbviewer.jupyter.org/github/fastai/fastai_docs/blob/master/dev/11_optimizer.ipynb#Optimizer--\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.zero_grad</code>()\n",
       "\n",
       "Zero all the grad attributes of the parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.zero_grad()\n",
    "[test_eq(p.grad, tensor([0.])) for p in params];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Optimizer.grad_params</code>\" class=\"doc_header\"><code>Optimizer.grad_params</code><a href=\"https://nbviewer.jupyter.org/github/fastai/fastai_docs/blob/master/dev/11_optimizer.ipynb#Optimizer--\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.grad_params</code>()\n",
       "\n",
       "Helper function to loop over param groups then params that have a grad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.grad_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used by `Optimizer.step` and `Optimizer.zero_grad` to loop first over the `param_groups` then over all the parameters that have a gradient, and return the tuples `(p,hyper)` where `hyper` is the dictionary of hyper-parameters associated to the parameter groups `p` is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "opt = Optimizer([params[:2], params[2:]], sgd_step, lr=0.1)\n",
    "opt.hypers[0]['lr'] = 0.01\n",
    "test_eq(opt.grad_params(), [(tensor([0.]), {'lr': 0.01}),\n",
    "                            (tensor([1.]), {'lr': 0.01}),\n",
    "                            (tensor([2.]), {'lr': 0.1}),\n",
    "                            (tensor([3.]), {'lr': 0.1})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatefulOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatefulOptimizer(Optimizer):\n",
    "    \"`Optimizer` that can have state through `stats`\"\n",
    "    def __init__(self, params, steppers, stats=None, **defaults): \n",
    "        self.stats = L(stats)\n",
    "        for stat in self.stats: defaults = {**getattr(stat, 'defaults', {}), **defaults}\n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {}\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update the stats and execute the steppers in on all parameters that have a grad\"\n",
    "        for p,hyper in self.grad_params():\n",
    "            state = self.state.get(p, {})\n",
    "            for stat in self.stats: state = stat(state, p, **hyper)\n",
    "            self.step_func(p, **state, **hyper)\n",
    "            self.state[p] = state\n",
    "    \n",
    "    def _init_state(self, p):\n",
    "        \"Create a state for p and call all the statistics to initialize it\"\n",
    "        state = {}\n",
    "        for stat in self.stats: state = {**getattr(stat, \"init_state\", lambda:{})(p), **state}\n",
    "        self.state[p] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between a `StatefulOptimizer` and an `Optimizer` is that a `StatefulOptimzier` keeps a state for things like moving averages of gradients. It does so with `stats` which basically are functions taking the state associated to a parameter, that parameter, plus the optimizer hyper-parameters and updates the state. That state can then be used by any stepper. It is initiliazed to an empty dictionary the first time we try to access it, then the `stat` function will have to properly initiliaze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_stat(state, p, **kwargs): \n",
    "    state['sum'] = state.get('sum', torch.zeros_like(p)) + p.data\n",
    "    return state\n",
    "tst_stat.defaults = {'mom': 0.9}\n",
    "\n",
    "#Test StatefulOptimizer init\n",
    "opt = StatefulOptimizer([1,2,3], noop, stats=tst_stat)\n",
    "test_eq(opt.hypers, [{'mom': 0.9}])\n",
    "opt = StatefulOptimizer([1,2,3], noop, stats=tst_stat, mom=0.99)\n",
    "test_eq(opt.hypers, [{'mom': 0.99}])\n",
    "\n",
    "#Test stat\n",
    "x = torch.randn(4,5)\n",
    "state = tst_stat({}, x)\n",
    "assert 'sum' in state\n",
    "test_eq(state['sum'], x)\n",
    "state = tst_stat(state, x)\n",
    "test_eq(state['sum'], 2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def average_grad(state, p, mom, dampening=False, **kwargs):\n",
    "    \"Keeps track of the avg grads of `p` in `state` with `mom`.\"\n",
    "    if 'grad_avg' not in state: state['grad_avg'] = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-mom if dampening else 1.\n",
    "    state['grad_avg'].mul_(mom).add_(damp, p.grad.data)\n",
    "    return state\n",
    "\n",
    "average_grad.defaults = dict(mom=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dampening=False` gives the classical formula for momentum in SGD: \n",
    "```\n",
    "new_val = old_val * mom + grad\n",
    "```\n",
    "whereas `dampening=True` makes it an exponential moving average:\n",
    "```\n",
    "new_val = old_val * mom + grad * (1-mom)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "state = {}\n",
    "state = average_grad(state, p, mom=0.9)\n",
    "test_eq(state['grad_avg'], p.grad)\n",
    "state = average_grad(state, p, mom=0.9)\n",
    "test_eq(state['grad_avg'], p.grad * 1.9)\n",
    "#Test dampening\n",
    "state = {}\n",
    "state = average_grad(state, p,  mom=0.9, dampening=True)\n",
    "test_eq(state['grad_avg'], 0.1*p.grad)\n",
    "state = average_grad(state, p, mom=0.9, dampening=True)\n",
    "test_eq(state['grad_avg'], (0.1*0.9+0.1)*p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def average_sqr_grad(state, p, sqr_mom, dampening=True, **kwargs):\n",
    "    if 'sqr_avg' not in state: state['sqr_avg'] = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    state['sqr_avg'].mul_(sqr_mom).addcmul_(damp, p.grad.data, p.grad.data)\n",
    "    return state\n",
    "\n",
    "average_sqr_grad.defaults = dict(sqr_mom=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dampening=False` gives the classical formula for momentum in SGD: \n",
    "```\n",
    "new_val = old_val * mom + grad**2\n",
    "```\n",
    "whereas `dampening=True` makes it an exponential moving average:\n",
    "```\n",
    "new_val = old_val * mom + (grad**2) * (1-mom)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "state = {}\n",
    "state = average_sqr_grad(state, p, sqr_mom=0.99, dampening=False)\n",
    "test_eq(state['sqr_avg'], p.grad.pow(2))\n",
    "state = average_sqr_grad(state, p, sqr_mom=0.99, dampening=False)\n",
    "test_eq(state['sqr_avg'], p.grad.pow(2) * 1.99)\n",
    "#Test dampening\n",
    "state = {}\n",
    "state = average_sqr_grad(state, p,  sqr_mom=0.99)\n",
    "test_close(state['sqr_avg'], 0.01*p.grad.pow(2))\n",
    "state = average_sqr_grad(state, p, sqr_mom=0.99)\n",
    "test_close(state['sqr_avg'], (0.01*0.99+0.01)*p.grad.pow(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    \"Step for SGD with momentum with `lr`\"\n",
    "    p.data.add_(-lr, grad_avg)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SGD(params, lr, mom=0., wd=0., true_wd=True):\n",
    "    \"A `StatefulOptimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    steppers = [] if wd==0. else [weight_decay] if true_wd else [l2_reg]\n",
    "    steppers.append(sgd_step if mom==0 else momentum_step)\n",
    "    if mom == 0.: return Optimizer(params, steppers, lr=lr, wd=wd)\n",
    "    else: return StatefulOptimizer(params, steppers, stats=average_grad, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `true_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vanilla SGD\n",
    "params = tst_params()\n",
    "opt = SGD(params, lr=0.1)\n",
    "assert not isinstance(opt, StatefulOptimizer)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "[p.item() for p in params]\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD with momentum\n",
    "params = tst_params()\n",
    "opt = SGD(params, lr=0.1, mom=0.9)\n",
    "assert isinstance(opt, StatefulOptimizer)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "[p.item() for p in params]\n",
    "test_close([p.item() for p in params], [i*(1 - 0.1 * (0.1 + 0.1*1.9)) for i in range(4)])\n",
    "for i,p in enumerate(params): test_close(opt.state[p]['grad_avg'].item(), i*0.19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test weight decay, notice how we can see that L2 regularization is different from weight decay even for simple SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "#Weight decay\n",
    "opt = SGD(params, lr=0.1, mom=0.9, wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])\n",
    "#L2 reg\n",
    "opt = SGD(params, lr=0.1, mom=0.9, wd=0.1, true_wd=False)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.97 for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n",
    "    \"Step for SGD with momentum with `lr`\"\n",
    "    denom = sqr_avg.sqrt().add_(eps)\n",
    "    p.data.addcdiv_(-lr, (grad_avg if grad_avg is not None else p.grad), denom)\n",
    "    return p\n",
    "\n",
    "rms_prop_step.defaults = dict(eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RMSProp(params, lr, sqr_mom=0.99, mom=0., wd=0., true_wd=True):\n",
    "    \"A `StatefulOptimizer` for RMSProp with `lr`, `sqr_mom`, `mom` and `params`\"\n",
    "    steppers = [] if wd==0. else [weight_decay] if true_wd else [l2_reg]\n",
    "    steppers.append(rms_prop_step)\n",
    "    stats = [average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad]\n",
    "    return StatefulOptimizer(params, steppers, stats=stats, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp was introduced by Geoffrey Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). What is named `sqr_mom` here is the `alpha` in the course. Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `true_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without momentum\n",
    "import math\n",
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = RMSProp(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.,1.,2.]))\n",
    "opt.step()\n",
    "step = - 0.1 * 0.1 / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([step, 1+step, 2+step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With momentum\n",
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = RMSProp(params, lr=0.1, mom=0.9)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.,1.,2.]))\n",
    "opt.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([step, 1+step, 2+step]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def step_stat(state, p, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    if 'step' not in state: state['step'] = 0\n",
    "    state['step'] += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1,0.1)\n",
    "state = {}\n",
    "state = step_stat(state, p)\n",
    "test_eq(state['step'], 1)\n",
    "for _ in range(5): state = step_stat(state, p)\n",
    "test_eq(state['step'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)\n",
    "\n",
    "def adam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for Adam with `lr` on `p`\"\n",
    "    debias1 = _debias(mom,     1-mom,     step)\n",
    "    debias2 = _debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
    "    return p\n",
    "\n",
    "adam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., true_wd=True):\n",
    "    \"A `StatefulOptimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    steppers = [] if wd==0. else [weight_decay] if true_wd else [l2_reg]\n",
    "    steppers.append(adam_step)\n",
    "    stats = [partial(average_grad, dampening=True), average_sqr_grad, step_stat]\n",
    "    return StatefulOptimizer(params, steppers, stats=stats, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam was introduced by Diederik P. Kingma and Jimmy Ba in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). For consistency accross optimizers, we renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that our defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from our experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `true_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    "> Note: Don't forget that `eps` is an hyper-parameter you can change. Some models won't train without a very high `eps` like 0.1 (intuitively, the higher `eps` is, the closer we are to normal SGD). The usual default of 1e-8 is often too extreme in the sense we don't manage to get as good results as with SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = Adam(params, lr=0.1)\n",
    "opt.step()\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([1+step, 2+step, 3+step]))\n",
    "opt.step()\n",
    "test_close(params[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS/LARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def larc_layer_lr(state, p, lr, trust_coeff, wd, eps, clip=True, **kwargs):\n",
    "    \"Computes the local lr before weight decay is applied\"\n",
    "    p_norm,g_norm = torch.norm(p.data),torch.norm(p.grad.data)\n",
    "    local_lr = lr*trust_coeff * (p_norm) / (g_norm + p_norm * wd + eps)\n",
    "    state['local_lr'] = min(lr, local_lr) if clip else local_lr\n",
    "    return state\n",
    "larc_layer_lr.defaults = dict(trust_coeff=0.02, wd=0., eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def larc_step(p, local_lr, grad_avg=None, **kwargs):\n",
    "    \"Step for LARC `local_lr` on `p`\"\n",
    "    p.data.add_(-local_lr, p.grad.data if grad_avg is None else grad_avg)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Larc(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., true_wd=True):\n",
    "    \"A `StatefulOptimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    steppers = [] if wd==0. else [weight_decay] if true_wd else [l2_reg]\n",
    "    steppers.append(larc_step)\n",
    "    stats = [] if mom==0. else [average_grad]\n",
    "    stats.append(partial(larc_layer_lr, clip=clip))\n",
    "    return StatefulOptimizer(params, steppers, stats=stats, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LARS optimizer was first introduced in [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888) then refined in its LARC variant (original LARS is with `clip=False`). A learning rate is computed for each individual layer with a certain `trust_coefficient`, then clipped to be always less than `lr`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `true_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt = Larc(params, lr=0.1)\n",
    "opt.step()\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt.state[params[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt.state[params[1]]['local_lr'], 0.1)\n",
    "test_close(params[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params[1], tensor([0.999,1.998,2.997]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt = Larc(params, lr=0.1, clip=False)\n",
    "opt.step()\n",
    "#No clipping\n",
    "test_close(opt.state[params[0]]['local_lr'], 0.02)\n",
    "test_close(opt.state[params[1]]['local_lr'], 0.2)\n",
    "test_close(params[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params[1], tensor([0.998,1.996,2.994]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lamb_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for LAMB with `lr` on `p`\"\n",
    "    debias1 = _debias(mom,     1-mom,     step)\n",
    "    debias2 = _debias(sqr_mom, 1-sqr_mom, step)\n",
    "    r1 = p.data.pow(2).mean().sqrt() #More stable numerically than taking the norm and we'll make a quotient\n",
    "    step = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps)\n",
    "    r2 = step.pow(2).mean().sqrt()\n",
    "    q = 1 if r1 == 0 or r2 == 0 else min(r1/r2,10)\n",
    "    p.data.add_(-lr * q, step)\n",
    "    return p\n",
    "lamb_step._defaults = dict(eps=1e-6, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., true_wd=True):\n",
    "    \"A `StatefulOptimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    steppers = [] if wd==0. else [weight_decay] if true_wd else [l2_reg]\n",
    "    steppers.append(lamb_step)\n",
    "    stats = [partial(average_grad, dampening=True), average_sqr_grad, step_stat]\n",
    "    return StatefulOptimizer(params, steppers, stats=stats, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMB was introduced in [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962). Intuitively, it's LARC applied to Adam. As in `Adam`, we renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that our defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from our experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `true_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = Lamb(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 02_data_pipeline.ipynb.\n",
      "Converted 03_data_external.ipynb.\n",
      "Converted 04_data_core.ipynb.\n",
      "Converted 05_data_source.ipynb.\n",
      "Converted 06_vision_core.ipynb.\n",
      "Converted 07_pets_tutorial.ipynb.\n",
      "Converted 10_layers.ipynb.\n",
      "Converted 11_optimizer.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from local.notebook.export import *\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
