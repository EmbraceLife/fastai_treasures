{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.core\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text core\n",
    "\n",
    "> Basic function to preprocess text before assembling it in a `DataBunch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Process, Pipe\n",
    "import spacy,html\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel(func, items, n_cpus=defaults.cpus):\n",
    "    \"Applies `func` in parallel to `items`, using `n_cpus`\"\n",
    "    if n_cpus<2: results = [func(o) for o in progress_bar(items, leave=False)]\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=n_cpus) as ex:\n",
    "            return [x for x in progress_bar(ex.map(func,items), total=len(items), leave=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(x): \n",
    "    time.sleep(random.random()/100)\n",
    "    return x+1\n",
    "\n",
    "test_eq(parallel(add_one, range(100)), range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class ProcessFunc():\n",
    "    \"A class for functions you want executed in `parallel_by_batch`\"\n",
    "    def iterate(self, batch): return (b for b in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def _wrap_process_func(pfunc_cls, batch, send_end, *args, start_idx=None, **kwargs):\n",
    "    f = pfunc_cls(*args, **kwargs)\n",
    "    if start_idx is not None:\n",
    "        for i,b in enumerate(f.iterate(batch)): send_end.send((start_idx+i,b))\n",
    "    else:\n",
    "        for b in f.iterate(batch): send_end.send(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 25, 50, 75, 99])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = np.array_split(range(99), 4)\n",
    "np.cumsum(0 + L(len(b) for b in batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel_by_batch(pfunc_cls, items, *args, n_cpus=defaults.cpus, enum_res=True, **kwargs):\n",
    "    \"Instantiate `pfunc_cls` in `n_cpus` process then call their iterate on batch of `items` in parallel.\"\n",
    "    recv_end, send_end = Pipe(False)\n",
    "    batches = np.array_split(items, n_cpus)\n",
    "    idx = np.cumsum(0 + L(len(b) for b in batches)) if enum_res else [None] * n_cpus\n",
    "    processes = [Process(target=_wrap_process_func, args=(pfunc_cls, b, send_end, *args), \n",
    "                         kwargs={'start_idx':i, **kwargs}) for b,i in zip (batches, idx)]\n",
    "    for p in processes: p.start()\n",
    "    for _ in progress_bar(items, leave=False): yield recv_end.recv()\n",
    "    for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pfunc_cls` should be a subclass of `ProcessFunc`. You can initiliaze its state in `__init__` (that will get passed `args` and `kwargs`. Note that `n_cpus` instances of `pfunc_cls` are created, one in each process. `items` are then split in `n_cpus` batches and one is sent to each `pfunc_cls.iterate`. The function then returns a generator over all the results, in the form of an enumerate if `enum_res=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning : As results don't come in the same order you pass your items (they come as they are put in the output queue by the various processes), the `enum_res` option allows you to track back the results to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretendLong(ProcessFunc):\n",
    "    def iterate(self, batch):\n",
    "        for k in batch:\n",
    "            time.sleep(random.random()/100)\n",
    "            yield k\n",
    "\n",
    "x = np.random.randn(100)\n",
    "res = []\n",
    "for k in parallel_by_batch(PretendLong, x, n_cpus=2): res.append(k)\n",
    "test_eq(set([a[0] for a in res]), set(range(100))) #Check we got all elements\n",
    "mapping = {a[0]:a[1] for a in res}\n",
    "test_eq([mapping[i] for i in range(100)], x) #Check we got x when remapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are rules applied to texts before or after it's tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxfld xxrep xxwrep xxup xxmaj\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_spec = re.compile(r'([/#\\\\])')\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return _re_spec.sub(r' \\1 ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(spec_add_spaces('#fastai'), ' # fastai')\n",
    "test_eq(spec_add_spaces('/fastai'), ' / fastai')\n",
    "test_eq(spec_add_spaces('\\\\fastai'), ' \\\\ fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_space = re.compile(' {2,}')\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(rm_useless_spaces('a  b   c'), 'a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    return _re_rep.sub(_replace_rep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same character or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_rep('aaa'), 'aaa')\n",
    "test_eq(replace_rep('aaaa'), f' {TK_REP} 4 a ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_wrep = re.compile(r'(?:\\s|^)(\\w+)\\s+((?:\\1\\s+){2,})\\1(\\s|\\W|$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Matches any word repeated at least four times with spaces between them\n",
    "```\n",
    "(?:\\s|^)          Non-catching group with either a whitespace character or the beginning of text\n",
    "(\\w+)             Catching group of any alphanumeric character\n",
    "\\s+               One or more whitespace\n",
    "((?:\\1\\s+){2,})   Catching group of a repetition of two or more times \\1 followed by one or more whitespace\n",
    "\\1                Occurence of \\1\n",
    "(\\s|\\W|$)         Catching group of last whitespace, non alphanumeric character or end of text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word word -> TK_WREP 4 word\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc,e = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+2} {c} {e}'\n",
    "    return _re_wrep.sub(_replace_wrep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same word or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_wrep('ah ah'), 'ah ah')\n",
    "test_eq(replace_wrep('ah ah ah ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah   ah  ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah ah ah '), f' {TK_WREP} 4 ah  ')\n",
    "test_eq(replace_wrep('ah ah ah ah.'), f' {TK_WREP} 4 ah .')\n",
    "test_eq(replace_wrep('ah ah ah ahi'), f'ah ah ah ahi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_html(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n",
    "        '#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(' @-@ ','-')\n",
    "    return html.unescape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fix_html('#39;bli#146;'), \"'bli'\")\n",
    "test_eq(fix_html('Sarah amp; Duck'), 'Sarah & Duck')\n",
    "test_eq(fix_html('a nbsp; #36;'), 'a   $')\n",
    "test_eq(fix_html('\\\\\" <unk>'), f'\" {UNK}')\n",
    "test_eq(fix_html('quot;  @.@  @-@ '), \"' .-\")\n",
    "test_eq(fix_html('<br />text\\\\n'), '\\ntext\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_all_caps = re.compile(r'(\\s|^)([A-Z]+[^a-z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any word in all caps, even with ' or - inside\n",
    "```\n",
    "(\\s|^)        Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]+       Catching group with one capitalized letter or more...\n",
    "[^a-z\\s]*)    ...followed by anything that's non lowercase or whitespace\n",
    "(?=(\\s|$))    Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_all_caps(m):\n",
    "        tok = f'{TK_UP} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_all_caps.sub(_replace_all_caps, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_all_caps(\"I'M SHOUTING\"), f\"{TK_UP} i'm {TK_UP} shouting\")\n",
    "test_eq(replace_all_caps(\"I'm speaking normally\"), \"I'm speaking normally\")\n",
    "test_eq(replace_all_caps(\"I am speaking normally\"), \"i am speaking normally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_maj = re.compile(r'(\\s|^)([A-Z][^A-Z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any capitalized word\n",
    "```\n",
    "(\\s|^)       Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]       Catching group with exactly one capitalized letter...\n",
    "[^A-Z\\s]*)   ...followed by anything that's not uppercase or whitespace\n",
    "(?=(\\s|$))   Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_maj(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_maj(m):\n",
    "        tok = f'{TK_MAJ} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_maj.sub(_replace_maj, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_maj(\"Jeremy Howard\"), f'{TK_MAJ} jeremy {TK_MAJ} howard')\n",
    "test_eq(replace_maj(\"I don't think there is any maj here\"), (\"i don't think there is any maj here\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lowercase(t, add_bos=True, add_eos=False):\n",
    "    \"Converts `t` to lowercase\"\n",
    "    return (f'{BOS} ' if add_bos else '') + t.lower().strip() + (f' {EOS}' if add_eos else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.text_spec_tok = [UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "defaults.text_proc_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces,\n",
    "                            replace_all_caps, replace_maj, lowercase]\n",
    "defaults.text_token_sep = '\\u2581'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is a class that must implement a `pipe` method. This `pipe` method receives a generator of texts and must return a generator with their tokenized versions. Here is the most basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseTokenizer():\n",
    "    \"Basic tokenizer that just splits on spaces\"\n",
    "    def __init__(self, **kwargs): pass\n",
    "    def pipe(self, items): \n",
    "        for t in items: yield t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BaseTokenizer()\n",
    "for t in tok.pipe([\"This is a text\"]): test_eq(t, [\"This\", \"is\", \"a\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpacyTokenizer():\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, batch_size=5000):\n",
    "        special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        self.nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in special_toks: self.nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def pipe(self, items):\n",
    "        for doc in self.nlp.pipe(items, batch_size=self.batch_size):\n",
    "            yield [d.text for d in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = SpacyTokenizer()\n",
    "for t in tok.pipe([\"This isn't the easiest text.\"]): \n",
    "    test_eq(t, [\"This\", \"is\", \"n't\", \"the\", \"easiest\", \"text\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_rules(items, rules):\n",
    "    \"Returns a generator that apply `rules`  to `items`\"\n",
    "    for o in items: yield compose(*rules)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in apply_rules([\"This is a text\"], [replace_maj]): test_eq(t, f\"{TK_MAJ} this is a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize1(text, tok_func=SpacyTokenizer, rules=None, **tok_kwargs):\n",
    "    \"Tokenize one `text` with an instance of `tok_func` and some `rules`\"\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules))\n",
    "    tokenizer = tok_func(**tok_kwargs)\n",
    "    for tok in tokenizer.pipe(apply_rules([text], rules)): return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tokenize1(\"This is a text\"), [BOS, TK_MAJ, 'this', 'is', 'a', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeBatch(ProcessFunc):\n",
    "    \"A wrapper around `tok_func` to apply `rules` and tokenize in parallel\"\n",
    "    def __init__(self, tok_func, rules, **tok_kwargs ): self.tok,self.rules = tok_func(**tok_kwargs),rules\n",
    "    def iterate(self, batch): return self.tok.pipe(apply_rules(batch, self.rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that will be called during one of the processes handling tokenization. It will create an instance of a tokenizer with `tok_func` and `tok_kwargs` at init, then iterate through the `batch` of texts, apply them `rules` and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"this is a text\", \"this is another text\"]\n",
    "rules = [lambda i: texts[i]]\n",
    "tok = TokenizeBatch(BaseTokenizer, rules)\n",
    "test_eq([t for t in tok.iterate([0,1])],[['this', 'is', 'a', 'text'], ['this', 'is', 'another', 'text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parallel_tokenize(items, tok_func, rules, **tok_kwargs):\n",
    "    \"Calls a potential setup on `tok_func` before launching `TokenizeBatch` in parallel\"\n",
    "    if hasattr(tok_func, 'setup'): tok_kwargs = tok_func(**tok_kwargs).setup(items, rules)\n",
    "    return parallel_by_batch(TokenizeBatch, items, tok_func, rules, **tok_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper will be very useful for tokenizer that require a first setup step (like sentencepiece)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_folders(path, output_dir, include=None):\n",
    "    \"Scan `path` and create the same folder architecture in `output_dir`\"\n",
    "    output_dir = Path(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "        if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "        else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "        for x in d: os.makedirs(output_dir/(Path(p)/Path(x)).relative_to(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `include` is specified, only the folders in `include` will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for k in range(i+1): os.makedirs(path/d/str(k), exist_ok=True)\n",
    "create_folders(path, 'tmp1')\n",
    "out = Path('tmp1')\n",
    "assert out.is_dir()\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    assert (out/d).is_dir()\n",
    "    for k in range(i+1): assert (out/d/str(k)).is_dir()\n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test with include\n",
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for k in range(i+1): os.makedirs(path/d/str(k), exist_ok=True)\n",
    "create_folders(path, 'tmp1', include=['b', 'c'])\n",
    "out = Path('tmp1')\n",
    "assert out.is_dir()\n",
    "for i,d in enumerate(['a', 'b', 'c']): \n",
    "    if i==0:\n",
    "        assert not (out/d).is_dir()\n",
    "        continue\n",
    "    assert (out/d).is_dir()\n",
    "    for k in range(i+1): assert (out/d/str(k)).is_dir()\n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function for texts in filenames. Tokenized texts will be saved in a similar fashion in a directory suffixed with `_tok` in the parent folder of `path` (override with `output_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_text(fname):\n",
    "    \"Read the content of `fname`\"\n",
    "    with open(fname, 'r') as f: return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_folder(path, extensions=None, include=None, output_dir=None, n_cpus=defaults.cpus,\n",
    "                    rules=None, tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize text files in `path` in parallel using `n_workers`\"\n",
    "    path = Path(path)\n",
    "    extensions = ifnone(extensions, ['.txt'])\n",
    "    fnames = get_files(path, extensions=extensions, recurse=True, include=include)\n",
    "    output_dir = Path(ifnone(output_dir, path.parent/f'{path.name}_tok'))\n",
    "    create_folders(path, output_dir, include=include)\n",
    "    rules = read_text + L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    counter = Counter()\n",
    "    \n",
    "    for i,tok in parallel_tokenize(fnames, tok_func, rules, **tok_kwargs):\n",
    "        out = output_dir/fnames[i].relative_to(path)\n",
    "        with open(out, 'w') as f: f.write(defaults.text_token_sep.join(tok))\n",
    "        with open(out.parent/f'{out.stem}.len', 'w') as f: f.write(str(len(tok)))\n",
    "        counter.update(Counter(tok))\n",
    "\n",
    "    pickle.dump(counter, open(output_dir/'counter.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be in `output_dir` (defaults to a folder in the same parent directory as `path`, with `_tok` added to `path.name`) with the same structure as in `path`. Tokenized texts for a given file will be in the file having the same name in `output_dir`. Additionally, a file with a .len suffix contains the number of tokens and the count of all words is stored in `output_dir/counter.pkl`.\n",
    "\n",
    "`extensions` will default to `['.txt']` and all text files in `path` are treated unless you specify a list of folders in `include`. `tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for d in ['a', 'b', 'c']: \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for i in range(5):\n",
    "        with open(path/d/f'text{i}.txt', 'w') as f: f.write(f\"This is an example of text {d} {i}\")\n",
    "tokenize_folder(path)\n",
    "out = Path('tmp_tok')\n",
    "assert out.is_dir()\n",
    "for d in ['a', 'b', 'c']: \n",
    "    assert (out/d).is_dir()\n",
    "    for i in range(5):\n",
    "        assert (out/d/f'text{i}.txt').is_file()\n",
    "        assert (out/d/f'text{i}.len').is_file()\n",
    "        test_eq(read_text(out/d/f'text{i}.txt'), defaults.text_token_sep.join([\n",
    "            BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', d, str(i)\n",
    "        ]))\n",
    "        test_eq(read_text(out/d/f'text{i}.len'), '10')\n",
    "        \n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _join_texts(df, mark_fields=False):\n",
    "    \"Join texts in row `idx` of `df`, marking each field with `FLD` if `mark_fields=True`\"\n",
    "    text_col = (f'{FLD} {1} ' if mark_fields else '' ) + df.iloc[:,0].astype(str)\n",
    "    for i in range(1,len(df.columns)):\n",
    "        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df.iloc[:,i].astype(str)\n",
    "    return text_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_df(df, text_cols, n_workers=defaults.cpus, rules=None, mark_fields=None, \n",
    "                tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize texts in `df[text_cols]` in parallel using `n_workers`\"\n",
    "    text_cols = L(text_cols)\n",
    "    mark_fields = ifnone(mark_fields, len(text_cols) > 1)\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    texts = _join_texts(df[text_cols], mark_fields=mark_fields)\n",
    "    lengths,outputs,counter = np.zeros(len(df)),np.zeros(len(df), dtype=np.object),Counter()\n",
    "    \n",
    "    for i,tok in parallel_tokenize(texts, tok_func, rules, **tok_kwargs):\n",
    "        lengths[i],outputs[i] = len(tok),defaults.text_token_sep.join(tok)\n",
    "        counter.update(Counter(tok))\n",
    "        \n",
    "    other_cols = [c for c in df.columns if c not in text_cols]\n",
    "    res = df[other_cols].copy()\n",
    "    res['text'],res['text_lengths'] = outputs,lengths\n",
    "    return res, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a new dataframe with the same non-text columns, a colum named text that contains the tokenized texts and a column named text_lengths that contains their respective length. It also returns a counter of all words see to quickly build a vocabulary afterward.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"This is an example of text {i}\" for i in range(10)]\n",
    "df = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\n",
    "out,cnt = tokenize_df(df, text_cols='text')\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], defaults.text_token_sep.join([\n",
    "        BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i)\n",
    "    ]))\n",
    "    test_eq(out['text_lengths'][i], 9)\n",
    "    \n",
    "#With two columns of text, mark_fields defaults to True\n",
    "df['text1'] = df['text'].values\n",
    "out,cnt = tokenize_df(df, text_cols=['text', 'text1'])\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], defaults.text_token_sep.join([\n",
    "        BOS, FLD, '1', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i),\n",
    "             FLD, '2', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i)\n",
    "    ]))\n",
    "    test_eq(out['text_lengths'][i], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: test + rework\n",
    "def tokenize_csv(fname, text_cols, outname=None, n_workers=4, rules=None, mark_fields=None, \n",
    "                 tok_func=SpacyTokenizer, header='infer', chunksize=None, **tok_kwargs):\n",
    "    \"Tokenize texts in the `text_cols` of the csv `fname` in parallel using `n_workers`\"\n",
    "    df = pd.read_csv(fname, header=header, chunksize=chunksize)\n",
    "    outname = Path(ifnone(outname, fname.parent/f'{fname.stem}_tok.csv'))\n",
    "    kwargs = dict(n_workers=n_workers, pre_rules=pre_rules, post_rules=post_rules, \n",
    "                  mark_fields=mark_fields, tok_func=tok_func, **tok_kwargs)\n",
    "    if chunksize is None:\n",
    "        out,cnt = tok_df(df, text_cols, **kwargs)\n",
    "        out.to_csv(outname, header=header, index=False)\n",
    "    else:\n",
    "        cnt = Counter()\n",
    "        for i,dfp in enumerate(df):\n",
    "            out,c = tok_df(dfp, text_cols, **kwargs)\n",
    "            out.to_csv(outname, header=header if i==0 else None, index=False, mode='w' if i==0 else 'a')\n",
    "            cnt.update(c)\n",
    "    pickle.dump(cnt, open(outname.parent/'counter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be written in a new csv file in `outname` (defaults to the same as `fname` with the suffix `_tok.csv`) and will have the same header as the original file, the same non-text columns, a text and a text_lengths column as described in `tokenize_df`.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field.\n",
    "\n",
    "The csv file is opened with `header` and optionally with blocks of `chunksize` at a time. If this argument is passed, each chunk is processed independtly and saved in the output file to save memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_langs = [\"bg\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"es\", \"et\", \"fi\", \"fr\", \"ga\", \"hr\", \"hu\",\n",
    "            \"it\",\"lt\",\"lv\",\"mt\",\"nl\",\"pl\",\"pt\",\"ro\",\"sk\",\"sl\",\"sv\"] # all European langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SentencePieceTokenizer():#TODO: pass the special tokens symbol to sp\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, \n",
    "                 model_type='unigram', char_coverage=None, cache_dir='tmp'):\n",
    "        try: from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n",
    "        except ImportError:\n",
    "            raise Exception('sentencepiece module is missing: run `pip install sentencepiece`')\n",
    "        self.sp_model,self.cache_dir = sp_model,Path(cache_dir)\n",
    "        self.vocab_sz,self.max_vocab_sz,self.model_type = vocab_sz,max_vocab_sz,model_type\n",
    "        self.char_coverage = ifnone(char_coverage, 0.99999 if lang in eu_langs else 0.9998)\n",
    "        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        if sp_model is None: self.tok = None\n",
    "        else:\n",
    "            self.tok = SentencePieceProcessor()\n",
    "            self.tok.Load(str(sp_model))\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def _get_vocab_sz(self, raw_text_path):\n",
    "        cnt = Counter()\n",
    "        with open(raw_text_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                cnt.update(line.split())\n",
    "                if len(cnt)//4 > self.max_vocab_sz: return self.max_vocab_sz\n",
    "        res = len(cnt)//4\n",
    "        while res%8 != 0: res+=1\n",
    "        return res\n",
    "    \n",
    "    def train(self, raw_text_path):\n",
    "        \"Train a sentencepiece tokenizer on `texts` and save it in `path/tmp_dir`\"\n",
    "        from sentencepiece import SentencePieceTrainer\n",
    "        vocab_sz = self._get_vocab_sz(raw_text_path) if self.vocab_sz is None else self.vocab_sz\n",
    "        spec_tokens = ['\\u2581'+s for s in self.special_toks]\n",
    "        SentencePieceTrainer.Train(\" \".join([\n",
    "            f\"--input={raw_text_path} --vocab_size={vocab_sz} --model_prefix={self.cache_dir/'spm'}\",\n",
    "            f\"--character_coverage={self.char_coverage} --model_type={self.model_type}\",\n",
    "            f\"--unk_id={len(spec_tokens)} --pad_id=-1 --bos_id=-1 --eos_id=-1\",\n",
    "            f\"--user_defined_symbols={','.join(spec_tokens)}\"]))\n",
    "        raw_text_path.unlink()\n",
    "        return self.cache_dir/'spm.model'\n",
    "    \n",
    "    def setup(self, items, rules):\n",
    "        if self.tok is not None: return {'sp_model': self.sp_model}\n",
    "        raw_text_path = self.cache_dir/'texts.out'\n",
    "        with open(raw_text_path, 'w') as f:\n",
    "            for t in progress_bar(apply_rules(items, rules), total=len(items), leave=False): \n",
    "                f.write(f'{t}\\n')\n",
    "        return {'sp_model': self.train(raw_text_path)}\n",
    "        \n",
    "    def pipe(self, items):\n",
    "        for t in items: yield self.tok.EncodeAsPieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"This is an example of text {i}\" for i in range(10)]\n",
    "df = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\n",
    "out,cnt = tokenize_df(df, text_cols='text', tok_func=SentencePieceTokenizer, vocab_sz=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  text_lengths\n",
       "0      0  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "1      1  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "2      2  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "3      3  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "4      4  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "5      5  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "6      6  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "7      7  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "8      8  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0\n",
       "9      9  ▁xxbos▁▁xxmaj▁▁▁t▁h▁i▁s▁▁▁i▁s▁▁▁a▁n▁▁▁e▁x▁a▁m▁...          31.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
