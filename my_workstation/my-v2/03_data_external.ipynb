{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External data\n",
    "> Helper functions to download the fastai datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_url(url, dest, overwrite=False, pbar=None, show_progress=True, chunk_size=1024*1024,\n",
    "                 timeout=4, retries=5):\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`\"\n",
    "    if os.path.exists(dest) and not overwrite: return\n",
    "    \n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    \n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = progress_bar(range(file_size), auto_update=False, leave=False, parent=pbar)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            fname = url.split('/')[-1]\n",
    "            from fastai.datasets import Config\n",
    "            data_dir = dest.parent\n",
    "            print(f'\\n Download of {url} has failed after {retries} retries\\n'\n",
    "                  f' Fix the download manually:\\n'\n",
    "                  f'$ mkdir -p {data_dir}\\n'\n",
    "                  f'$ cd {data_dir}\\n'\n",
    "                  f'$ wget -c {url}\\n'\n",
    "                  f'$ tar -zxvf {fname}\\n'\n",
    "                  f' And re-run your code once the download is successful\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, fname = 'http://files.fast.ai/data/examples/mnist_tiny.tgz', Path('mnist_tiny.tgz')\n",
    "download_url(url, fname)\n",
    "assert fname.exists()\n",
    "t = os.path.getmtime(fname)\n",
    "#Launching the function again doesn't trigger a download since the file is already there.\n",
    "download_url(url, fname)\n",
    "test_eq(t, os.path.getmtime(fname))\n",
    "#But with the overwrite option, we download it again.\n",
    "download_url(url, fname, overwrite=True)\n",
    "test_ne(t, os.path.getmtime(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class URLs():\n",
    "    \"Global constants for dataset and model URLs.\"\n",
    "    LOCAL_PATH = Path.cwd()\n",
    "    URL = 'http://files.fast.ai/data/examples/'\n",
    "    MDL = 'http://files.fast.ai/models/'\n",
    "    S3 = 'https://s3.amazonaws.com/fast-ai-'\n",
    "\n",
    "    S3_IMAGE    = f'{S3}imageclas/'\n",
    "    S3_IMAGELOC = f'{S3}imagelocal/'\n",
    "    S3_NLP      = f'{S3}nlp/'\n",
    "    S3_COCO     = f'{S3}coco/'\n",
    "    S3_MODEL    = f'{S3}modelzoo/'\n",
    "\n",
    "    # main datasets\n",
    "    ADULT_SAMPLE        = f'{URL}adult_sample.tgz'\n",
    "    BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'\n",
    "    CIFAR               = f'{URL}cifar10.tgz'\n",
    "    COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'\n",
    "    COCO_TINY           = f'{URL}coco_tiny.tgz'\n",
    "    HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'\n",
    "    IMDB                = f'{S3_NLP}imdb.tgz'\n",
    "    IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'\n",
    "    ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'\n",
    "    MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'\n",
    "    MNIST_TINY          = f'{URL}mnist_tiny.tgz'\n",
    "    MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n",
    "    PLANET_SAMPLE       = f'{URL}planet_sample.tgz'\n",
    "    PLANET_TINY         = f'{URL}planet_tiny.tgz'\n",
    "    IMAGENETTE          = f'{S3_IMAGE}imagenette.tgz'\n",
    "    IMAGENETTE_160      = f'{S3_IMAGE}imagenette-160.tgz'\n",
    "    IMAGENETTE_320      = f'{S3_IMAGE}imagenette-320.tgz'\n",
    "    IMAGEWOOF           = f'{S3_IMAGE}imagewoof.tgz'\n",
    "    IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof-160.tgz'\n",
    "    IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof-320.tgz'\n",
    "\n",
    "    # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\n",
    "    DOGS = f'{URL}dogscats.tgz'\n",
    "\n",
    "    # image classification datasets\n",
    "    CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n",
    "    CARS         = f'{S3_IMAGE}stanford-cars.tgz'\n",
    "    CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'\n",
    "    CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n",
    "    FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'\n",
    "    FOOD         = f'{S3_IMAGE}food-101.tgz'\n",
    "    MNIST        = f'{S3_IMAGE}mnist_png.tgz'\n",
    "    PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n",
    "\n",
    "    # NLP datasets\n",
    "    AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'\n",
    "    AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'\n",
    "    AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n",
    "    DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'\n",
    "    MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'\n",
    "    SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'\n",
    "    WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'\n",
    "    WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'\n",
    "    YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'\n",
    "    YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'\n",
    "    YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n",
    "\n",
    "    # Image localization datasets\n",
    "    BIWI_HEAD_POSE     = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n",
    "    CAMVID             = f'{S3_IMAGELOC}camvid.tgz'\n",
    "    CAMVID_TINY        = f'{URL}camvid_tiny.tgz'\n",
    "    LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'\n",
    "    PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'\n",
    "    PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'\n",
    "\n",
    "    #Pretrained models\n",
    "    OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'\n",
    "    WT103              = f'{S3_MODEL}wt103.tgz'\n",
    "    #TODO: remove this last one and make sure the mosr recent is up\n",
    "    WT103_1            = f'{S3_MODEL}wt103-1.tgz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_config():\n",
    "    config_path = Path(os.getenv('FASTAI_HOME', '~/.fastai')).expanduser()\n",
    "    config_file = config_path/'config.yml'\n",
    "    if config_file.exists(): \n",
    "        with open(config_file, 'r') as yaml_file: \n",
    "            config = yaml.safe_load(yaml_file)\n",
    "            if 'version' in config and config['version'] == 1: return config\n",
    "    else: config = {}\n",
    "    #File inexistent or wrong version -> going to default\n",
    "    config = {'data_path':    str(config_path/'data'),\n",
    "              'archive_path': str(config_path/'archive'),\n",
    "              'model_path':   str(config_path/'models'),\n",
    "              'version':      1} \n",
    "    with open(config_file, 'w') as yaml_file: \n",
    "        yaml.dump(config, yaml_file, default_flow_style=False)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is just to make the config file compatible with current fastai\n",
    "config = _get_config()\n",
    "if 'data_archive_path' not in config: config['data_archive_path'] = config['data_path']\n",
    "config_path = Path(os.getenv('FASTAI_HOME', '~/.fastai')).expanduser()\n",
    "config_file,config_bak = config_path/'config.yml',config_path/'config.yml.bak'\n",
    "with open(config_file, 'w') as yaml_file: \n",
    "    yaml.dump(config, yaml_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(os.getenv('FASTAI_HOME', '~/.fastai')).expanduser()\n",
    "config_file,config_bak = config_path/'config.yml',config_path/'config.yml.bak'\n",
    "if config_file.exists(): shutil.move(config_file, config_bak)\n",
    "#Test default config\n",
    "config = _get_config()\n",
    "assert (config_path/'config.yml').exists()\n",
    "test_eq(config, {\n",
    "        'data_path':    str(config_path/'data'),\n",
    "        'archive_path': str(config_path/'archive'),\n",
    "        'model_path':   str(config_path/'models'),\n",
    "        'version':      1\n",
    "    })\n",
    "\n",
    "#Test change in config\n",
    "config['archive_path'] = '.'\n",
    "with open(config_path/'config.yml', 'w') as yaml_file: \n",
    "    yaml.dump(config, yaml_file, default_flow_style=False)\n",
    "config = _get_config()\n",
    "test_eq(config, {\n",
    "        'data_path':    str(config_path/'data'),\n",
    "        'archive_path': '.',\n",
    "        'model_path':   str(config_path/'models'),\n",
    "        'version':      1\n",
    "    })\n",
    "\n",
    "if config_bak.exists(): shutil.move(config_bak, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ConfigKey = Enum('ConfigKey', 'Data Archive Model')\n",
    "\n",
    "def get_path(c_key=ConfigKey.Data):\n",
    "    return Path(_get_config()[f\"{c_key.name.lower()}_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = _get_config()\n",
    "test_eq(Path(config['data_path']),    get_path(ConfigKey.Data))\n",
    "test_eq(Path(config['archive_path']), get_path(ConfigKey.Archive))\n",
    "test_eq(Path(config['model_path']),   get_path(ConfigKey.Model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download in the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _url2path(url, c_key=ConfigKey.Archive):\n",
    "    fname = url.split('/')[-1]\n",
    "    local_path = URLs.LOCAL_PATH/('models' if c_key==ConfigKey.Model else 'data')/fname\n",
    "    if local_path.exists(): return local_path\n",
    "    return get_path(c_key)/fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path,path_bak = URLs.LOCAL_PATH/'data',URLs.LOCAL_PATH/'data1'\n",
    "if path.exists(): shutil.move(path, path_bak)\n",
    "test_eq(_url2path(URLs.MNIST_TINY), get_path(ConfigKey.Archive)/'mnist_tiny.tgz')\n",
    "test_eq(_url2path(URLs.MNIST_TINY.replace('tgz', 'tar')), get_path(ConfigKey.Archive)/'mnist_tiny.tar')\n",
    "test_eq(_url2path(URLs.MNIST_TINY,c_key=ConfigKey.Model), get_path(ConfigKey.Model)/'mnist_tiny.tgz')\n",
    "if path_bak.exists(): shutil.move(path_bak, path)\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "download_url(f\"{URLs.MNIST_TINY}.tgz\", 'data/mnist_tiny.tgz')\n",
    "test_eq(_url2path(URLs.MNIST_TINY), Path.cwd()/'data'/'mnist_tiny.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_data(url, fname=None, c_key=ConfigKey.Archive, force_download=False):\n",
    "    \"Download `url` to `fname`.\"\n",
    "    fname = Path(fname or _url2path(url, c_key=c_key))\n",
    "    fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not fname.exists() or force_download:\n",
    "        print(f'Downloading {url}')\n",
    "        download_url(url, fname, overwrite=force_download)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `fname` is None, it will default to the archive folder you have in your config file (or data, model if you specify a different `c_key`) followed by the last part of the url: for instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `fname` will be `~/.fastai/archive/mnist_sample.tgz`.\n",
    "\n",
    "If `force_download=True`, the file is alwayd downloaded. Otherwise, it's only when the file doesn't exists that the download is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(download_data(URLs.MNIST_SAMPLE), get_path(ConfigKey.Archive)/'mnist_sample.tgz')\n",
    "test_eq(download_data(URLs.MNIST_TINY, fname=Path('mnist.tgz')), Path('mnist.tgz'))\n",
    "os.remove(Path('mnist.tgz'))\n",
    "\n",
    "tst_model = get_path(ConfigKey.Model)/'mnist_tiny.tgz'\n",
    "test_eq(download_data(URLs.MNIST_TINY, c_key=ConfigKey.Model), tst_model)\n",
    "os.remove(tst_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check datasets -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Tricking jupyter notebook to have a __file__ attribute. All _file_ will be replaced by __file__\n",
    "_file_ = Path('local').absolute()/'data'/'external.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_check(url):\n",
    "    checks = json.load(open(Path(_file_).parent/'checks.txt', 'r'))\n",
    "    return checks.get(url, None)\n",
    "\n",
    "def _check_file(fname):\n",
    "    size = os.path.getsize(fname)\n",
    "    with open(fname, \"rb\") as f:\n",
    "        hash_nb = hashlib.md5(f.read(2**20)).hexdigest()\n",
    "    return [size,hash_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(_get_check(URLs.MNIST_SAMPLE), _check_file(_url2path(URLs.MNIST_SAMPLE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_check(url, fname):\n",
    "    \"Internal function to update the internal check file with `url` and check on `fname`.\"\n",
    "    checks = json.load(open(Path(_file_).parent/'checks.txt', 'r'))\n",
    "    checks[url] = _check_file(fname)\n",
    "    json.dump(checks, open(Path(_file_).parent/'checks.txt', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def untar_data(url, fname=None, dest=None, c_key=ConfigKey.Data, force_download=False):\n",
    "    \"Download `url` to `fname` if `dest` doesn't exist, and un-tgz to folder `dest`.\"\n",
    "    default_dest = _url2path(url, c_key=c_key).with_suffix('')\n",
    "    dest = default_dest if dest is None else Path(dest)/default_dest.name\n",
    "    fname = Path(fname or _url2path(url))\n",
    "    if fname.exists() and _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "        print(\"A new version of this is available, downloading...\")\n",
    "        force_download = True\n",
    "    if force_download:\n",
    "        if fname.exists(): os.remove(fname)\n",
    "        if dest.exists(): shutil.rmtree(dest)\n",
    "    if not dest.exists():\n",
    "        fname = download_data(url, fname=fname, c_key=c_key)\n",
    "        if _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "            print(f\"File downloaded is broken. Remove {fname} and try again.\")\n",
    "        tarfile.open(fname, 'r:gz').extractall(dest.parent)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`untar_data` is a convenience function for the fastai datasets, intended to work with the urls in `URLs`. You can use it with another url only if it ends with `.tgz` (otherwise the function can download it but not decompress it). For other extensions, you should use `download_data` then the necessary decompress function.\n",
    "\n",
    "If `fname` is specified, the data will be downloaded to that destination, otherwise it will default to the archive path in your config file (default `~/.fastai/archive/`) followed by the last part of your url. For instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `fname` will be `~/.fastai/archive/mnist_sample.tgz`.\n",
    "\n",
    "If `dest` is specified, the data will be decompressed to that folder. Otherwise, it will default to the data path (or model/archive if you specify a different `c_key`) in your config file (default `~/.fastai/data/`) followed by the last part of your url without extension. For instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `dest` will be `~/.fastai/data/mnist_sample`.\n",
    "\n",
    "`force_download=True` will retrigger a download, otherwise the behavior is to:\n",
    "- not do anything when `dest` exists\n",
    "- otherwise decompress `fname` to `dest` if `fname` exists\n",
    "- otherwise download then decompress `fname` to `dest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(untar_data(URLs.MNIST_SAMPLE), get_path()/'mnist_sample')\n",
    "\n",
    "#Test specific fname\n",
    "untar_data(URLs.MNIST_TINY, fname='mnist_tiny.tgz', force_download=True)\n",
    "assert Path('mnist_tiny.tgz').exists()\n",
    "os.remove(Path('mnist_tiny.tgz'))\n",
    "\n",
    "#Test specific dest\n",
    "test_eq(untar_data(URLs.MNIST_TINY, dest='.'), Path('mnist_tiny'))\n",
    "assert Path('mnist_tiny').exists()\n",
    "shutil.rmtree(Path('mnist_tiny'))\n",
    "\n",
    "#Test c_key\n",
    "tst_model = get_path(ConfigKey.Model)/'mnist_sample'\n",
    "test_eq(untar_data(URLs.MNIST_SAMPLE, c_key=ConfigKey.Model), tst_model)\n",
    "assert not tst_model.with_suffix('.tgz').exists() #Archive wasn't downloaded in the models path\n",
    "assert (get_path(ConfigKey.Archive)/'mnist_sample.tgz').exists() #Archive was downloaded there\n",
    "shutil.rmtree(tst_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 02_data_pipeline.ipynb.\n",
      "Converted 03_data_external.ipynb.\n",
      "Converted 04_data_core.ipynb.\n",
      "Converted 05_data_source.ipynb.\n",
      "Converted 06_vision_core.ipynb.\n",
      "Converted 07_pets_tutorial.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "! ./notebook2script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
