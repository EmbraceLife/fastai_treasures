{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战深度学习2019\n",
    "\n",
    "作者：24 Jan 2019 by *Jeremy Howard* \n",
    "翻译：深度碎片\n",
    "\n",
    "就在今天，2019年版的[实战深度学习](http://course.fast.ai)上线了。这是本课程的第三次迭代，而且是100%新内容，其中包括了之前版本从未涉及的新应用案例（以及还未在任何学术期刊中发表过的新技术技巧）。本课程有7节课，每课约2小时，每节课的作业你需要准备约10小时的投入。[Google Cloud](https://course.fast.ai/start_gcp.html) 和 [Microsoft Azure](https://course.fast.ai/start_azure.html) 的GPU平台已经为你作为本课程所需的所有设置, 甚至还有一键即用的GPU平台可以选择，如[Crestle](https://course.fast.ai/start_crestle.html) 和 [Gradient](https://course.fast.ai/start_gradient.html).\n",
    "\n",
    "\n",
    "本课假设你拥有至少一年的编程经验（最好是Python, 当然资深的编程者可以边听课边学Python; 我们提供了一个[Python资源清单](https://forums.fast.ai/t/recommended-python-learning-resources/26888))，以及掌握高中数学（一些大学数学内容会在课程中介绍讲解）。许多学员完成课程后告诉我们说，完成课程学习的工作量很大，但也是他/她们所做的收获最大的事情之一。要成功学完本课，我们强烈推荐大家融入到课程的[在线社区](https://forums.fast.ai/c/part1-v3)中来。\n",
    "\n",
    "\n",
    "第一节课后你将能用自己的数据训练一个最先进的图片分类模型。在完成第一课时后，一些现场实体课程（录制版本）学员发表了不同领域里的最新最强的模型表现！本课前半部分的重点是*实战技巧*， 只会展示这些技巧所涉及的*理论*。课程的后半部分，我们会深入理论，直到最后一课，我们会手动构建并训练一个*resnet*神经网络模型，并达到最先进的精度效果。\n",
    "\n",
    "\n",
    "![](images/lesson1-overview.png)\n",
    "课程中的一些应用案例\n",
    "\n",
    "核心应用领域包括：\n",
    "\n",
    "* 机器视觉 (例如，对图片中的不同种类的宠物分类)\n",
    "\n",
    "\t* 图片分类\n",
    "\t* 图片定位 (图片像素分割和激 segmentation and activation maps)\n",
    "\t* 图片关键点定位\n",
    "\n",
    "* NLP自然语言处理 (例如，影评情绪分析)\n",
    "\n",
    "\t* 语言模型\n",
    "\t* 文本分类\n",
    "\n",
    "* 表格数据建模 Tabular data (例如，销售额预测)\n",
    "\n",
    "\t* 类别数据 Categorical data\n",
    "\t* 连续数据 Continuous data\n",
    "\n",
    "* 协同过滤 (例如，电影推荐)\n",
    "\n",
    "我们也会覆盖到这些应用所需的全部基础知识点。\n",
    "\n",
    "![](images/lesson7-topics.png)\n",
    "本课所覆盖的基础知识点\n",
    "\n",
    "我们用[PyTorch](https://pytorch.org/)和 [fastai](https://github.com/fastai/fastai)库来辅助教学。Pytorch，是当前最先进和灵活的库，fastai则是基于PyTorch搭建的库，目的是能更轻松快捷使用公认最优的深度学习技术技巧（同时能还直接调用所有pytorch底层功能)。我们认为fastai非常棒，但我们的观点是不公正的，因为我们创造了它...但它是唯一一个被Pytorch.org专题推荐的通用深度学习工具箱，并且拥有超过10,000 GitHub 点赞，同时还用于帮助众多竞赛获胜，大量在学术论文和顶级大学课程中使用，所以不是只有我们喜欢fastai！ 注意：你所学的概念适用于所有深度学习库，如Tensorflow/keras, CNTK, MXNet, 等等。关键的是概念，毕竟学习一个新库只需要几天时间，如果你已经理解了概念的话。\n",
    "\n",
    "今年新增的一个尤其有用的工具是一个超强大的视频播放器，这要感谢[Zach Caceres](http://zachcaceres.com/now/)。这个播放器允许你搜索课程字幕文本，并跳跃到你指定的文字所在。它也提供其他课程视频链接，以及每节课的总结和资源，通过可开合的边框来看，(但目前手机版不太好用，所以如果你想在手机上看，可以用 [这个youtube播放列表](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn) ). 此外还要特别感谢[Sylvain Gugger](https://sgugger.github.io/)，在课程和fastai库的建设上贡献巨大。我们还要感谢Amazon Web Services对Sylvain的项目赞助。\n",
    "\n",
    "![](images/videos.png)\n",
    "fast.ai的视频播放器以及其可搜索时间轴\n",
    "\n",
    "如果你想尝试，[点击这里](http://course.fast.ai) 前往课程页面。现在让我们仔细看看每一课的内容。\n",
    "\n",
    "## 第一课：图片分类\n",
    "\n",
    "第一课的最重要的成果是，我们将训练出一个SOTA精度级别的宠物类别图片分类模型。这个成果的关键是*迁移学习*，这也是构成本课大部分内容的一个基础性的重要构件。我们还回如何分析模型来理解预测失败情况。在这个案例中，我们还会看到模型所犯的错误是宠物专家也无法幸免的。\n",
    "\n",
    "![](images/lesson1.png)\n",
    "训练和分析一个宠物分类器\n",
    "\n",
    "我们还将讨论本课的教学方法，采用了不同寻常的*自上而下*的模式。不是先理论后实践，我们一上来就是实战应用，然后逐步深挖这些应用案例，理论只在实际需要情况下才学习。这个教学模式需要老师花费更多心血去开发，但效果显著，David Perkins 在[education research at Harvard](https://www.gse.harvard.edu/news/uk/09/01/education-bat-seven-principles-educators)中讲述了案例。\n",
    "\n",
    "我们还会讨论在训练神经网络时，如何设置最重要的*超参数*：*学习率*，并且是采用Leslie Smith的超酷*learning rate finder*方法。最后，我们还要了解非常重要但极少谈及的*标注*问题，并学习fastai提供的超好用的帮助你标注图片数据的工具。\n",
    "\n",
    "注意：要使用课程Notebook，你需要有\n",
    "跟随课程，你需要有一个云端GPU能运行fastai（推荐，目前最便宜的是每小时0.5美元），或者在本地设置自己的GPU（非常费时费事，不推荐）。你需要熟悉Jupyter Notebook的使用环境来做深度学习实验。更多最新的GPU指南可以在[课程官网](http://course.fast.ai)中查看。\n",
    "\n",
    "\n",
    "## Lesson 2: Data cleaning and production; SGD from scratch\n",
    "\n",
    "We start today’s lesson by learning how to build your own image classification model using your own data, including topics such as:\n",
    "\n",
    "* Image collection\n",
    "* Parallel downloading\n",
    "* Creating a validation set, and\n",
    "* Data cleaning, using the model to help us find data problems.\n",
    "I’ll demonstrate all these steps as I create a model that can take on the vital task of differentiating teddy bears from grizzly bears. Once we’ve got our data set in order, we’ll then learn how to productionize our teddy-finder, and make it available online.\n",
    "\n",
    "![](lesson3-b.png)\n",
    "Putting your model in production\n",
    "\n",
    "We’ve had some great additions since this lesson was recorded, so be sure to check out:\n",
    "\n",
    "* The *production starter kits* on the course web site, such as [this one](https://course.fast.ai/deployment_render.html) for deploying to Render.com\n",
    "* The new interactive GUI in the lesson notebook for using the model to find and fix mislabeled or incorrectly-collected images.\n",
    "In the second half of the lesson we’ll train a simple model from scratch, creating our own *gradient descent* loop. In the process, we’ll be learning lots of new jargon, so be sure you’ve got a good place to take notes, since we’ll be referring to this new terminology throughout the course (and there will be lots more introduced in every lesson from here on).\n",
    "\n",
    "![](lesson2.gif)\n",
    "Gradient descent in action\n",
    "\n",
    "## Lesson 3: Data blocks; Multi-label classification; Segmentation\n",
    "\n",
    "Lots to cover today! We start lesson 3 looking at an interesting dataset: Planet’s [Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space). In order to get this data into the shape we need it for modeling, we’ll use one of fastai’s most powerful (and unique!) tools: the [data block API](https://docs.fast.ai/data_block.html). We’ll be coming back to this API many times over the coming lessons, and mastery of it will make you a real fastai superstar! Once you’ve finished this lesson, if you’re ready to learn more about the data block API, have a look at this great article: [Finding Data Block Nirvana](https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4), by Wayde Gilliam.\n",
    "\n",
    "One important feature of the Planet dataset is that it is a *multi-label* dataset. That is: each satellite image can contain *multiple* labels, whereas previous datasets we’ve looked at have had exactly one label per image. We’ll look at what changes we need to make to work with multi-label datasets.\n",
    "\n",
    "![](lesson3-camvid.png)\n",
    "The result of our image segmentation model\n",
    "\n",
    "Next, we will look at *image segmentation*, which is the process of labeling every pixel in an image with a category that shows what kind of object is portrayed by that pixel. We will use similar techniques to the earlier image classification models, with a few tweaks. fastai makes image segmentation modeling and interpretation just as easy as image classification, so there won’t be too many tweaks required.\n",
    "\n",
    "We will be using the popular CamVid dataset for this part of the lesson. In future lessons, we will come back to it and show a few extra tricks. Our final CamVid model will have dramatically lower error than any model we’ve been able to find in the academic literature!\n",
    "\n",
    "What if your dependent variable is a continuous value, instead of a category? We answer that question next, looking at a [keypoint](https://stackoverflow.com/questions/29133085/what-are-keypoints-in-image-processing) dataset, and building a model that predicts face keypoints with precision.\n",
    "\n",
    "## Lesson 4: NLP; Tabular data; Collaborative filtering; Embeddings\n",
    "\n",
    "In lesson 4 we’ll dive into *natural language processing* (NLP), using the IMDb movie review dataset. In this task, our goal is to predict whether a movie review is positive or negative; this is called *sentiment analysis*. We’ll be using the [ULMFiT](https://arxiv.org/abs/1801.06146) algorithm, which was originally developed during the fast.ai 2018 course, and became part of a revolution in NLP during 2018 which led the New York Times to declare that [new systems are starting to crack the code of natural language](https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html). ULMFiT is today the most accurate known sentiment analysis algorithm.\n",
    "\n",
    "![](lesson4-ulmfit.png)\n",
    "Overview of ULMFiT\n",
    "\n",
    "The basic steps are:\n",
    "\n",
    "1. Create (or, preferred, download a pre-trained) *language model* trained on a large corpus such as Wikipedia (a “language model” is any model that learns to predict the next word of a sentence)\n",
    "2. Fine-tune this language model using your *target corpus* (in this case, IMDb movie reviews)\n",
    "3. Remove the *encoder* in this fine tuned language model, and replace it with a *classifier*. Then fine-tune this model for the final classification task (in this case, sentiment analysis).\n",
    "After our journey into NLP, we’ll complete our practical applications for Practical Deep Learning for Coders by covering tabular data (such as spreadsheets and database tables), and collaborative filtering (recommendation systems).\n",
    "\n",
    "For tabular data, we’ll see how to use *categorical* and *continuous* variables, and how to work with the *fastai.tabular* module to set up and train a model.\n",
    "\n",
    "Then we’ll see how collaborative filtering models can be built using similar ideas to those for tabular data, but with some special tricks to get both higher accuracy and more informative model interpretation.\n",
    "\n",
    "This brings us to the half-way point of the course, where we have looked at how to build and interpret models in each of these key application areas:\n",
    "\n",
    "* Computer vision\n",
    "* NLP\n",
    "* Tabular\n",
    "* Collaborative filtering\n",
    "For the second half of the course, we’ll learn about *how* these models really work, and how to create them ourselves from scratch. For this lesson, we’ll put together some of the key pieces we’ve touched on so far:\n",
    "\n",
    "* Activations\n",
    "* Parameters\n",
    "* Layers (affine and non-linear)\n",
    "* Loss function.\n",
    "We’ll be coming back to each of these in lots more detail during the remaining lessons. We’ll also learn about a type of layer that is important for NLP, collaborative filtering, and tabular models: the *embedding layer*. As we’ll discover, an “embedding” is simply a computational shortcut for a particular type of matrix multiplication (a multiplication by a *one-hot encoded* matrix).\n",
    "\n",
    "## Lesson 5: Back propagation; Accelerated SGD; Neural net from scratch\n",
    "\n",
    "In lesson 5 we put all the pieces of training together to understand exactly what is going on when we talk about *back propagation*. We’ll use this knowledge to create and train a simple neural network from scratch.\n",
    "\n",
    "![](lesson5.png)\n",
    "Neural net from scratch\n",
    "\n",
    "We’ll also see how we can look inside the weights of an embedding layer, to find out what our model has learned about our categorical variables. This will let us get some insights into which movies we should probably avoid at all costs…\n",
    "\n",
    "![](lesson5-emb.png)\n",
    "Interpreting movie review embeddings\n",
    "\n",
    "Although embeddings are most widely known in the context of word embeddings for NLP, they are at least as important for categorical variables in general, such as for tabular data or collaborative filtering. They can even be used with non-neural models with great success.\n",
    "\n",
    "![](lesson5-b.png)\n",
    "Comparative performance of common models with vs without embeddings\n",
    "\n",
    "## Lesson 6: Regularization; Convolutions; Data ethics\n",
    "\n",
    "Today we discuss some powerful techniques for improving training and avoiding over-fitting:\n",
    "\n",
    "* **Dropout**: remove activations at random during training in order to regularize the model\n",
    "* **Data augmentation**: modify model inputs during training in order to effectively increase data size\n",
    "* **Batch normalization**: adjust the parameterization of a model in order to make the loss surface smoother.\n",
    "\n",
    "![](lesson6.png)\n",
    "Data augmentation examples for a single image\n",
    "\n",
    "Next up, we’ll learn all about *convolutions*, which can be thought of as a variant of matrix multiplication with tied weights, and are the operation at the heart of modern computer vision models (and, increasingly, other types of models too).\n",
    "\n",
    "We’ll use this knowledge to create a *class activated map*, which is a heat-map that shows which parts of an image were most important in making a prediction.\n",
    "\n",
    "![](lesson6-cnn.png)\n",
    "How a convolution works\n",
    "\n",
    "Finally, we’ll cover a topic that many students have told us is the most interesting and surprising part of the course: data ethics. We’ll learn about some of the ways in which models can go wrong, with a particular focus on *feedback loops*, why they cause problems, and how to avoid them. We’ll also look at ways in which bias in data can lead to biased algorithms, and discuss questions that data scientists can and should be asking to help ensure that their work doesn’t lead to unexpected negative outcomes.\n",
    "\n",
    "![](lesson6-ethics.png)\n",
    "Example of algorithmic bias in the US justice system\n",
    "\n",
    "## Lesson 7: Resnets from scratch; U-net; Generative (adversarial) networks\n",
    "\n",
    "In the final lesson of Practical Deep Learning for Coders we’ll study one of the most important techniques in modern architectures: the *skip connection*. This is most famously used in the *resnet*, which is the architecture we’ve used throughout this course for image classification, and appears in many cutting-edge results. We’ll also look at the *U-net* architecture, which uses a different type of skip connection to greatly improve segmentation results (and also for similar tasks where the output structure is similar to the input).\n",
    "\n",
    "![](lesson7-resnet.png)\n",
    "Impact on loss surface of resnet skip connections\n",
    "\n",
    "We’ll then use the U-net architecture to train a *super-resolution* model. This is a model which can increase the resolution of a low-quality image. Our model won’t only increase resolution—it will also remove jpeg artifacts and unwanted text watermarks.\n",
    "\n",
    "In order to make our model produce high quality results, we will need to create a custom loss function which incorporates *feature loss* (also known as *perceptual loss* ), along with *gram loss*. These techniques can be used for many other types of image generation task, such as image colorization.\n",
    "\n",
    "![](lesson7.png)\n",
    "Super-resolution results using feature loss and gram loss\n",
    "\n",
    "We’ll learn about a recent loss function known as *generative adversarial* loss (used in generative adversarial networks, or *GANs* ), which can improve the quality of generative models in some contexts, at the cost of speed.\n",
    "\n",
    "The techniques we show in this lesson include some unpublished research that:\n",
    "\n",
    "* Let us train GANs more quickly and reliably than standard approaches, by leveraging transfer learning\n",
    "* Combines architectural innovations and loss function approaches that haven’t been used in this way before.\n",
    "The results are stunning, and train in just a couple of hours (compared to previous approaches that take a couple of days).\n",
    "\n",
    "![](lesson7-rnn.png)\n",
    "A recurrent neural net\n",
    "\n",
    "Finally, we’ll learn how to create a recurrent neural net (RNN) from scratch. This is the foundation of the models we have been using for NLP throughout the course, and it turns out they are a simple refactoring of a regular multi-layer network.\n",
    "\n",
    "Thanks for reading! If you’ve gotten this far, then you should probably head over to [course.fast.ai](http://course.fast.ai) and start watching the first video!\n",
    "\n",
    "This post is tagged: [ [courses](/tag/courses) ] (click a tag for more posts in that category). \n",
    "\n",
    "## Related Posts\n",
    "\n",
    "### [fast.ai Embracing Swift for Deep Learning 06 Mar 2019](/2019/03/06/fastai-swift/)\n",
    "\n",
    "### [A Conversation about Tech Ethics with the New York Times Chief Data Scientist 04 Mar 2019](/2019/03/04/ethics-framework/)\n",
    "\n",
    "### [Dairy farming, solar panels, and diagnosing Parkinson's disease: what can you do with deep learning? 21 Feb 2019](/2019/02/21/dl-projects/)\n",
    "\n",
    "[](https://www.fast.ai/2019/01/24/course-v3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
